{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Случайные леса\n__Суммарное количество баллов: 10__\n\nВ этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным и сравнить его эффективность с ансамблями из самых популярных библиотек.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nimport matplotlib\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom statistics import mode\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom itertools import product\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:40:31.212878Z","iopub.status.busy":"2022-03-09T05:40:31.21256Z","iopub.status.idle":"2022-03-09T05:40:32.343185Z","shell.execute_reply":"2022-03-09T05:40:32.342142Z","shell.execute_reply.started":"2022-03-09T05:40:31.212803Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание. Используйте реализацию дерева из HW3.\n\n#### Параметры конструктора\n`n_estimators` - количество используемых для предсказания деревьев.\n\nОстальное - параметры деревьев.\n\n#### Методы\n`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n\n`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья.","metadata":{}},{"cell_type":"markdown","source":"Возьмем дерево из sklearn для ускорения работы случайного леса.","metadata":{}},{"cell_type":"code","source":"class RandomForestClassifier:\n    def __init__(self, criterion=\"gini\", max_depth=3, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n        self.n_estimators = n_estimators\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        \n        # для подсчета out-of-bag\n        self.err_oob = []\n        self.err_oob_j = {}\n        \n        # будем хранить построенные деревья в списке\n        self.forest = []\n    \n    def fit(self, X, y):\n        for n in range(self.n_estimators):\n            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=\"auto\",\n                                          min_samples_leaf=self.min_samples_leaf, criterion=self.criterion)\n            X_fit, y_fit = resample(X, y)\n            tree.fit(X_fit, y_fit)\n            self.forest.append(tree)\n        \n            err_oob_idx = set(X.index) - set(X_fit.index)\n            self.err_oob.append(accuracy_score(y.loc[err_oob_idx], tree.predict(X.loc[err_oob_idx])))\n            \n            # Перемешать значения признака j у объектов выборки\n            for col in X.columns:\n                X_err = X.loc[err_oob_idx].copy()\n                X_err[col] = np.array(resample(X_err[col]))\n                \n                if col not in self.err_oob_j:\n                    self.err_oob_j[col] = [accuracy_score(y.loc[err_oob_idx], tree.predict(X_err))]\n                    \n                else:\n                    self.err_oob_j[col].append(self.err_oob[-1] - accuracy_score(y.loc[err_oob_idx], tree.predict(X_err)))\n    \n    def predict(self, X_test):\n        forest_predicts = np.array([tree.predict(X_test) for tree in self.forest]).T\n        forest_proba =  np.array([tree.predict_proba(X_test) for tree in self.forest]).T\n        mode_choise = []\n        \n        for tree in forest_predicts:\n            choise = []\n            tr = Counter(tree)\n            choise.append(max(tr, key=tr.get))\n            mode_choise.append(max(choise))\n        return mode_choise\n    \n    def importance(self, X):\n        temp = {}\n        \n        for key in self.err_oob_j:\n            temp[key] = sum(self.err_oob_j[key]) / len(self.err_oob_j[key])\n            \n        return sorted(list(temp.items()), key=lambda x: x[1], reverse=True)","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:40:32.386607Z","iopub.status.busy":"2022-03-09T05:40:32.386274Z","iopub.status.idle":"2022-03-09T05:40:32.40625Z","shell.execute_reply":"2022-03-09T05:40:32.405759Z","shell.execute_reply.started":"2022-03-09T05:40:32.38658Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \nОптимизируйте по `AUC` на кроссвалидации (размер валидационной выборки - 20%) параметры своей реализации `Random Forest`: \n\nмаксимальную глубину деревьев из [2, 3, 5, 7, 10], количество деревьев из [5, 10, 20, 30, 50, 100]. \n\nПостройте `ROC` кривую (и выведите `AUC` и `accuracy`) для лучшего варианта.\n\nПодсказка: можно построить сразу 100 деревьев глубины 10, а потом убирать деревья и\nглубину.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_train.csv')\nX = df.drop(columns='Id')\ny = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/y_spam_train.csv')\ny = y['Expected']\np = {'max_depth' : [2, 3, 5, 7, 10], 'n_estimators' : [5, 10, 20, 30, 50, 100]}\nforest = RandomForestClassifier()\nkeys = p.keys()\niters = product(*p.values())\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nfor param in iters:\n    forest.__init__(**{key : value for key, value in zip(keys, param)})\n    forest.fit(X_train, y_train)\n    print(param, roc_auc_score(y_test, forest.predict(X_test)))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:40:32.408208Z","iopub.status.busy":"2022-03-09T05:40:32.407847Z","iopub.status.idle":"2022-03-09T05:40:41.074633Z","shell.execute_reply":"2022-03-09T05:40:41.073443Z","shell.execute_reply.started":"2022-03-09T05:40:32.408173Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В качестве параметров возьмем глубину 3 и количество деревьев 40.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_train.csv')\nX = df.drop(columns='Id')\ny = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/y_spam_train.csv')\ny = y['Expected']\nforest = RandomForestClassifier(max_depth=3, n_estimators=40)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T05:40:41.075925Z","iopub.status.idle":"2022-03-09T05:40:41.076441Z","shell.execute_reply":"2022-03-09T05:40:41.076255Z","shell.execute_reply.started":"2022-03-09T05:40:41.076233Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest.fit(X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(y_test, p_pred):\n    fpr, tpr, thresholds = roc_curve(y_test, p_pred)\n    plt.figure(figsize = (7, 7))\n    plt.plot(fpr, tpr)\n    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    plt.xlim(-0.01, 1.01)\n    plt.ylim(-0.01, 1.01)\n    plt.tight_layout()\n    plt.show()\n    \ny_pred =  forest.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T05:40:41.077386Z","iopub.status.idle":"2022-03-09T05:40:41.078102Z","shell.execute_reply":"2022-03-09T05:40:41.077909Z","shell.execute_reply.started":"2022-03-09T05:40:41.077885Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(y_test, y_pred)\nprint('AUC:', roc_auc_score(y_test, y_pred))\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1:', f1_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest выглядит так:\n1. Посчитать out-of-bag ошибку предсказания `err_oob` (https://en.wikipedia.org/wiki/Out-of-bag_error)\n2. Перемешать значения признака `j` у объектов выборки (у каждого из объектов изменится значение признака `j` на какой-то другой)\n3. Посчитать out-of-bag ошибку (`err_oob_j`) еще раз.\n4. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n\nРеализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака.","metadata":{}},{"cell_type":"code","source":"def feature_importance(rfc):\n    return rfc.importance(X)\n\ndef most_important_features(importance, names, k=20):\n    # Выводит названия k самых важных признаков\n    idicies = np.argsort(importance)[::-1][:k]\n    return np.array(names)[idicies]","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:40:51.909235Z","iopub.status.busy":"2022-03-09T05:40:51.908966Z","iopub.status.idle":"2022-03-09T05:40:51.914646Z","shell.execute_reply":"2022-03-09T05:40:51.913909Z","shell.execute_reply.started":"2022-03-09T05:40:51.909204Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Протестируйте решение на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем.","metadata":{}},{"cell_type":"code","source":"def synthetic_dataset(size):\n    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n    y = [i % 3 for i in range(size)]\n    return np.array(X), np.array(y)\n\nX, y = synthetic_dataset(1000)\ny = pd.DataFrame(y, columns = ['Expected'])\nX = pd.DataFrame(X)\n\nrfc = RandomForestClassifier(n_estimators=10)\nrfc.fit(X, y)\nprint(\"Accuracy:\", np.mean(rfc.predict(X) == y['Expected']))\nprint(\"Importance:\", feature_importance(rfc))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:40:53.824613Z","iopub.status.busy":"2022-03-09T05:40:53.824238Z","iopub.status.idle":"2022-03-09T05:40:59.486454Z","shell.execute_reply":"2022-03-09T05:40:59.485661Z","shell.execute_reply.started":"2022-03-09T05:40:53.824584Z"},"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверьте, какие признаки важны для датасета spam? (Используйте файлы x_spam_train и y_spam_train)","metadata":{}},{"cell_type":"markdown","source":"_Ваш ответ_","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_train.csv')\nX = df.drop(columns='Id')\ny = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/y_spam_train.csv')\ny = y['Expected']\nrfc = RandomForestClassifier(n_estimators=3)\nrfc.fit(X, y)\nprint(\"Importance:\", feature_importance(rfc))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T04:22:15.58558Z","iopub.status.busy":"2022-03-09T04:22:15.58526Z","iopub.status.idle":"2022-03-09T04:28:05.717387Z","shell.execute_reply":"2022-03-09T04:28:05.716477Z","shell.execute_reply.started":"2022-03-09T04:22:15.585531Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Обучите модель на всех данных из x_spam_train и y_spam_train.\n2. Сделайте submit своего решения и получите значение f1_score не менее 0.6","metadata":{}},{"cell_type":"code","source":"# кросс-валидация\n\ndf = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_train.csv')\nX = df.drop(columns='Id')\ny = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/y_spam_train.csv')\ny = y['Expected']\n\nn = len(y)\nn1 = int(np.ceil(n / 3))\nn2 = int(np.ceil( 2 * n / 3))\nn3 = n\n\nforest = RandomForestClassifier(max_depth=3, n_estimators=40)\n\nforest.fit(X.loc[range(0, n2)], y.loc[range(0, n2)])\ny1 = forest.predict(X.loc[range(n2, n3)])\nprint(accuracy_score(y.loc[range(n2, n3)], y1))\n\nx2 = X.loc[range(n1, n2)]\n\nindex = list(set(X.index) - set(x2.index))\n\nforest.fit(X.loc[index], y.loc[index])\ny2 = forest.predict(X.loc[range(n1, n2)])\nprint(accuracy_score(y.loc[range(n1, n2)], y2))\n      \nforest.fit(X.loc[range(n2, n3)], y.loc[range(n2, n3)])\ny3 = forest.predict(X.loc[range(0, n1)])\nprint(accuracy_score(y.loc[range(0, n1)], y3))\n\n# обучение на всех данных\n\nforest.fit(X, y)","metadata":{"execution":{"iopub.execute_input":"2022-03-09T05:48:47.579235Z","iopub.status.busy":"2022-03-09T05:48:47.578696Z","iopub.status.idle":"2022-03-09T05:49:52.872305Z","shell.execute_reply":"2022-03-09T05:49:52.871181Z","shell.execute_reply.started":"2022-03-09T05:48:47.579208Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_train.csv')\nX = df.drop(columns='Id')\ny = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/y_spam_train.csv')\ny = y['Expected']\nX_spam_test = pd.read_csv('/kaggle/input/homework-ensembles-ib-22/x_spam_test.csv')\nsubmission = pd.DataFrame(columns = [\"Id\", \"Expected\"])\nsubmission[\"Id\"] = X_spam_test['Id']\nX_spam_test = X_spam_test.drop(columns='Id')\ntest = forest.predict(X_spam_test)\nsubmission[\"Expected\"] = test\nsubmission.to_csv('submission.csv', index=False)","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В качестве альтернативы попробуем библиотечные реализации ансамблей моделей. \n\n1. [CatBoost](https://catboost.ai/docs/)\n2. [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n3. [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n","metadata":{}},{"cell_type":"markdown","source":"Установите необходимые библиотеки. \nВозможно, потребуется установка дополнительных пакетов.","metadata":{}},{"cell_type":"markdown","source":"1. Примените модели для нашего датасета.\n\n2. Для стандартного набора параметров у каждой модели нарисуйте `ROC` кривую и выведите `AUC` и `accuracy`.\n\n3. Посчитайте время обучения каждой модели (можно использовать [timeit magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)).\n\n4. Сравните метрики качества и скорость обучения моделей. Какие выводы можно сделать?","metadata":{}},{"cell_type":"markdown","source":"_Ваш ответ_","metadata":{}},{"cell_type":"code","source":"%%time\nfrom catboost import CatBoostClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95)\nclf = CatBoostClassifier(silent=True)\nclf.fit(X_train, y_train)\ny_pred =  clf.predict(X_test)\nplot_roc_curve(y_test, y_pred)\nprint('AUC:', roc_auc_score(y_test, y_pred))\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1:', f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T04:35:55.286712Z","iopub.status.busy":"2022-03-09T04:35:55.286123Z","iopub.status.idle":"2022-03-09T04:35:57.054455Z","shell.execute_reply":"2022-03-09T04:35:57.053774Z","shell.execute_reply.started":"2022-03-09T04:35:55.286664Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom xgboost import XGBClassifier\nclf = XGBClassifier()\nclf.fit(X_train, y_train)\ny_pred =  clf.predict(X_test)\nplot_roc_curve(y_test, y_pred)\nprint('AUC:', roc_auc_score(y_test, y_pred))\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1:', f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T04:13:43.252589Z","iopub.status.busy":"2022-03-09T04:13:43.252287Z","iopub.status.idle":"2022-03-09T04:13:43.398013Z","shell.execute_reply":"2022-03-09T04:13:43.397098Z","shell.execute_reply.started":"2022-03-09T04:13:43.252546Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMClassifier\nclf = LGBMClassifier()\nclf.fit(X_train, y_train)\ny_pred =  clf.predict(X_test)\nplot_roc_curve(y_test, y_pred)\nprint('AUC:', roc_auc_score(y_test, y_pred))\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1:', f1_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoostClassifier дольше всех обучается, но имеет самую высокую метрику F1. В целом все три модели достаточно похожи","metadata":{}}]}